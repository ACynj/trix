# 基于下游任务表现的预训练配置优化指南

## 📊 当前表现分析

基于54个下游数据集的评估结果：

- **平均MRR**: 0.7939
- **优秀表现** (MRR >= 0.9): 14个数据集
- **良好表现** (0.8 <= MRR < 0.9): 13个数据集
- **一般表现** (0.6 <= MRR < 0.8): 23个数据集
- **较差表现** (MRR < 0.6): 4个数据集

### 表现较差的数据集
1. **Metafam**: MRR 0.3304
2. **AristoV4**: MRR 0.4261
3. **NELL995**: MRR 0.5147
4. **NELLInductive(v1)**: MRR 0.5713

## 🎯 优化目标

1. **提升平均MRR**: 从0.79 → 0.82+
2. **改善较差数据集**: 将4个较差数据集的MRR提升到0.6+
3. **保持优秀表现**: 确保14个优秀数据集不下降

## 🔧 优化策略

### 核心问题分析

1. **beta值可能过高** (当前1e-3)
   - 限制了机制z的灵活性
   - 导致某些数据集（如NELL995, AristoV4）表现差

2. **训练轮次可能不足**
   - 当前10个epoch可能不够充分收敛
   - 需要更多轮次让机制z充分学习

3. **需要更好的正则化**
   - 添加weight_decay防止过拟合
   - 保持模型泛化能力

## 📝 推荐配置

### 配置1：平衡优化 (推荐) ⭐
**文件**: `pretrain_optimized_v1.yaml`

```yaml
beta: 2.5e-4      # 降低KL约束，提升灵活性
lr: 4.0e-4        # 适中学习率
weight_decay: 1.0e-5
num_epoch: 25     # 增加训练轮次
```

**预期效果**:
- 平均MRR: 0.79 → **0.81-0.82**
- 较差数据集: 0.33-0.57 → **0.50-0.65**
- 训练时间: ~8-9小时

**适用场景**: 平衡性能和训练时间

---

### 配置2：激进优化 🚀
**文件**: `pretrain_optimized_v2.yaml`

```yaml
beta: 1.5e-4      # 更低的KL约束
lr: 3.5e-4        # 较低学习率
weight_decay: 1.5e-5
num_epoch: 30     # 最长训练
```

**预期效果**:
- 平均MRR: 0.79 → **0.82-0.83**
- 较差数据集: 0.33-0.57 → **0.55-0.70**
- 训练时间: ~10-11小时

**适用场景**: 追求最佳性能，不介意训练时间

---

### 配置3：精细调优 🎯
**文件**: `pretrain_optimized_v3.yaml`

```yaml
beta: 3.0e-4      # 轻微降低
lr: 4.0e-4
weight_decay: 1.0e-5
num_epoch: 20     # 适度增加
```

**预期效果**:
- 平均MRR: 0.79 → **0.80-0.81**
- 较差数据集: 0.33-0.57 → **0.45-0.60**
- 训练时间: ~7-8小时

**适用场景**: 保守优化，小幅提升

## 🚀 使用建议

### 第一步：运行配置1（平衡优化）
```bash
python src/pretrain_relation.py \
    -c ./config/pretrain_optimized_v1.yaml \
    --gpus [0]
```

**如果平均MRR达到0.81+，说明策略有效！**

### 第二步：如果效果好，尝试配置2（激进优化）
```bash
python src/pretrain_relation.py \
    -c ./config/pretrain_optimized_v2.yaml \
    --gpus [0]
```

**目标：平均MRR > 0.82，较差数据集MRR > 0.60**

## 📈 关键优化点总结

| 配置项 | 当前值 | 优化值 | 理由 |
|--------|--------|--------|------|
| **beta** | 1.0e-3 | **2.5e-4** | 降低KL约束，提升机制z灵活性，改善较差数据集 |
| **lr** | 5.0e-4 | **4.0e-4** | 适中学习率，配合更长训练 |
| **weight_decay** | 无 | **1.0e-5** | 防止过拟合，保持泛化能力 |
| **num_epoch** | 10 | **25** | 增加训练轮次，充分收敛 |

## 🎯 预期最终效果

基于当前分析和优化策略：

- **平均MRR**: 0.79 → **0.81-0.83** (+2-4%)
- **较差数据集**: 4个 → **0-2个** (大部分提升到0.6+)
- **优秀数据集**: 14个 → **15-16个** (保持或增加)

**这将是一个显著的改进！**

## ⚠️ 注意事项

1. **模型类型**: 当前配置使用 `TRIXLatentMechanism` (query-free版本)
   - 如果要用预训练版本，改为 `TRIXLatentMechanismPretrain`

2. **beta值权衡**:
   - 太低：可能失去正则化效果
   - 太高：限制机制z灵活性
   - 推荐范围：1.5e-4 到 3.0e-4

3. **训练时间**:
   - 配置1: ~8-9小时
   - 配置2: ~10-11小时
   - 配置3: ~7-8小时



