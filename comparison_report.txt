====================================================================================================
实验结果对比报告 - MRR 和 Hits@10
====================================================================================================

基准方法: TRIX (inference_rel.log)
新方法1: Inductive (run_commands_20251225_041027.log)
新方法2: Transductive (run_commands_20251225_065140.log + 133504.log)

====================================================================================================

【归纳数据集 (Inductive) 对比】
----------------------------------------------------------------------------------------------------
数据集                                      基准MRR        新方法MRR       ΔMRR       基准H@10       新方法H@10      ΔH@10     
----------------------------------------------------------------------------------------------------
FB15k237Inductive(v1)                    0.68946      0.66269      -0.0268    0.89051      0.90024      +0.0097
FB15k237Inductive(v2)                    0.69409      0.68649      -0.0076    0.91552      0.91130      -0.0042
FB15k237Inductive(v3)                    0.72664      0.70639      -0.0203    0.89312      0.91623      +0.0231
FB15k237Inductive(v4)                    0.74508      0.72193      -0.0231    0.91056      0.92782      +0.0173
FBIngram(100)                            0.92408      0.91209      -0.0120    0.96952      0.97381      +0.0043
FBIngram(25)                             0.79242      0.79034      -0.0021    0.91743      0.92950      +0.0121
FBIngram(50)                             0.76482      0.75660      -0.0082    0.88837      0.91751      +0.0291
FBIngram(75)                             0.81925      0.79136      -0.0279    0.92434      0.92949      +0.0052
FBNELL(None)                             0.75459      0.75372      -0.0009    0.92295      0.93970      +0.0168
HM(1k)                                   0.61331      0.68783      ++0.0745    1.00000      1.00000      +0.0000
HM(3k)                                   0.63235      0.68994      ++0.0576    1.00000      1.00000      +0.0000
HM(5k)                                   0.65309      0.70022      ++0.0471    1.00000      1.00000      +0.0000
HM(indigo)                               0.84161      0.82216      -0.0194    0.98141      0.95880      -0.0226
ILPC2022(large)                          0.89369      0.85351      -0.0402    0.97349      0.98144      +0.0080
ILPC2022(small)                          0.91463      0.90548      -0.0091    0.97657      0.98794      +0.0114
Metafam(None)                            0.30273      0.33040      ++0.0277    1.00000      1.00000      +0.0000
NELLInductive(v1)                        0.55535      0.57131      ++0.0160    1.00000      1.00000      +0.0000
NELLInductive(v2)                        0.79665      0.80079      ++0.0041    0.91979      0.91123      -0.0086
NELLInductive(v3)                        0.74742      0.73778      -0.0096    0.93395      0.93704      +0.0031
NELLInductive(v4)                        0.80664      0.79752      -0.0091    0.96337      0.94679      -0.0166
NLIngram(0)                              0.66044      0.65555      -0.0049    0.88598      0.89384      +0.0079
NLIngram(100)                            0.89876      0.88680      -0.0120    0.99369      0.99369      +0.0000
NLIngram(25)                             0.75706      0.75140      -0.0057    0.93011      0.93280      +0.0027
NLIngram(50)                             0.79018      0.77668      -0.0135    0.93597      0.93364      -0.0023
NLIngram(75)                             0.78358      0.79763      ++0.0140    0.92751      0.94728      +0.0198
WKIngram(100)                            0.89215      0.92037      ++0.0282    0.96397      0.96864      +0.0047
WKIngram(25)                             0.90575      0.93118      ++0.0254    0.96110      0.98055      +0.0195
WKIngram(50)                             0.86775      0.91064      ++0.0429    0.95070      0.96837      +0.0177
WKIngram(75)                             0.93086      0.93046      -0.0004    0.96329      0.97290      +0.0096
WN18RRInductive(v1)                      0.80846      0.83326      ++0.0248    1.00000      1.00000      +0.0000
WN18RRInductive(v2)                      0.76327      0.80968      ++0.0464    1.00000      1.00000      +0.0000
WN18RRInductive(v3)                      0.71788      0.82668      ++0.1088    1.00000      1.00000      +0.0000
WN18RRInductive(v4)                      0.76423      0.78106      ++0.0168    1.00000      1.00000      +0.0000
WikiTopicsMT1(health)                    0.93073      0.90598      -0.0248    1.00000      1.00000      +0.0000
WikiTopicsMT1(tax)                       0.97275      0.99305      ++0.0203    1.00000      1.00000      +0.0000
WikiTopicsMT2(org)                       0.98369      0.98532      ++0.0016    1.00000      1.00000      +0.0000
WikiTopicsMT2(sci)                       0.94948      0.98358      ++0.0341    0.99818      0.99818      +0.0000
WikiTopicsMT3(art)                       0.89572      0.87756      -0.0182    0.96884      0.97045      +0.0016
WikiTopicsMT3(infra)                     0.92210      0.95136      ++0.0293    0.99667      0.99875      +0.0021
WikiTopicsMT4(health)                    0.91882      0.95709      ++0.0383    0.99765      0.99883      +0.0012
WikiTopicsMT4(sci)                       0.94229      0.97004      ++0.0277    0.99496      0.99784      +0.0029

归纳数据集统计: 共41个数据集
  MRR提升: 20个, 下降: 21个
  Hits@10提升: 22个, 下降: 5个


【转导数据集 (Transductive) 对比】
----------------------------------------------------------------------------------------------------
数据集                                      基准MRR        新方法MRR       ΔMRR       基准H@10       新方法H@10      ΔH@10     
----------------------------------------------------------------------------------------------------
AristoV4(None)                           0.38533      0.42611      ++0.0408    0.65310      0.66395      +0.0109
CoDExLarge(None)                         0.90006      0.85771      -0.0424    0.98312      0.98877      +0.0056
CoDExSmall(None)                         0.96024      0.93625      -0.0240    0.99289      0.99289      +0.0000
ConceptNet100k(None)                     0.63494      0.64245      ++0.0075    0.98667      0.98917      +0.0025
DBpedia100k(None)                        0.69059      0.73989      ++0.0493    0.91668      0.93632      +0.0196
FB15k237_10(None)                        0.77784      0.78315      ++0.0053    0.90937      0.92876      +0.0194
FB15k237_20(None)                        0.81588      0.81839      ++0.0025    0.94589      0.94812      +0.0022
FB15k237_50(None)                        0.86688      0.84950      -0.0174    0.97363      0.95892      -0.0147
Hetionet(None)                           0.82524      0.89930      ++0.0741    0.99736      0.99966      +0.0023
NELL23k(None)                            0.73692      0.77177      ++0.0349    0.90388      0.92670      +0.0228
NELL995(None)                            0.54419      0.51468      -0.0295    0.77537      0.74840      -0.0270
WDsinger(None)                           0.68216      0.71100      ++0.0288    0.83386      0.90558      +0.0717
YAGO310(None)                            0.74538      0.80493      ++0.0596    0.97280      0.99500      +0.0222

转导数据集统计: 共13个数据集
  MRR提升: 9个, 下降: 4个
  Hits@10提升: 10个, 下降: 2个


====================================================================================================
【分析建议】
====================================================================================================

1. 归纳数据集表现:
   ✓ 最佳提升: WN18RRInductive(v3) (MRR: +0.1088, H@10: +0.0000)
   ✗ 最大下降: ILPC2022(large) (MRR: -0.0402, H@10: +0.0080)

2. 转导数据集表现:
   ✓ 最佳提升: Hetionet(None) (MRR: +0.0741, H@10: +0.0023)
   ✗ 最大下降: CoDExLarge(None) (MRR: -0.0424, H@10: +0.0056)

3. 归纳数据集平均变化:
   平均MRR变化: +0.0095
   平均Hits@10变化: +0.0043

4. 转导数据集平均变化:
   平均MRR变化: +0.0146
   平均Hits@10变化: +0.0106

5. 改进建议:
   - 对于表现下降的数据集，建议:
     * 降低beta值（从1e-3降到1e-4或5e-4）
     * 检查数据集规模，大规模数据集可能需要更小的beta
     * 考虑数据集特定的超参数调优
   - 对于表现提升的数据集，建议:
     * 保持当前配置
     * 分析提升原因，看是否可以应用到其他数据集

====================================================================================================
