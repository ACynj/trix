# 预训练结果深度分析

## 关键发现

### WN18RR 的突破性表现
- **Epoch 0**: MRR 0.690233
- **Epoch 9**: MRR **0.720215** (+4.3% 提升！)
- **最终测试**: MRR 0.71622

**结论**: WN18RR 需要更多训练轮次才能充分发挥机制 z 的优势！

### 各数据集最佳表现

| 数据集 | 最佳 Epoch | MRR | 说明 |
|--------|-----------|-----|------|
| FB15k237 | Epoch 1 | 0.910392 | 早期收敛，后期略降 |
| WN18RR | **Epoch 9** | **0.720215** | 需要长期训练 |
| CoDExMedium | Epoch 0 | 0.940161 | 早期最好，后期下降 |

### 训练趋势分析

1. **WN18RR**: 持续上升（0.69 → 0.72），说明：
   - beta=1e-3 在长期训练中有效
   - 需要更多轮次让机制 z 充分学习
   - 可能受益于更低的 beta 和更长训练

2. **FB15k237**: Epoch 1 后下降，说明：
   - 可能过拟合
   - 需要正则化或学习率调整

3. **CoDExMedium**: Epoch 0 后下降，说明：
   - 早期收敛快
   - 需要防止过拟合

## 最有潜力的配置策略

基于分析，最有潜力的配置应该：

1. **降低 beta** (2.5e-4 或 5e-4)：让机制 z 更灵活，可能进一步提升 WN18RR
2. **增加训练轮次** (20-25)：让 WN18RR 充分收敛
3. **添加权重衰减**：防止 FB15k237 和 CoDExMedium 过拟合
4. **使用 TRIXLatentMechanismPretrain**：保持原始 query 初始化（预训练用）



