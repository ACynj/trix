# 预训练配置优化说明 (v4 & v5)

## 问题分析

根据对比分析，第二次实验（使用epoch 14 checkpoint）相比第一次实验（使用rel_5.pth）存在以下问题：

### 整体问题
- **MR上升5.95%** - 平均排名变差
- **Hits@3下降1.14%** - 前3命中率下降
- **MRR仅提升0.18%** - 提升不明显

### 分类问题
1. **Transductive数据集（13个）**: 表现相对较好，MRR提升0.85%，Hits@1提升1.40%
2. **Inductive数据集（18个）**: 表现最差
   - MR上升12.56%（严重变差）
   - Hits@3下降2.79%
   - 提升5个，下降11个
3. **Ingram数据集（23个）**: 表现略有下降
   - MR上升5.69%
   - MRR下降0.34%
   - Hits@3下降0.97%

## 优化策略

### 1. 降低beta值（机制正则化强度）
- **当前配置**: beta = 2.5e-4
- **v4配置**: beta = 1.0e-4（降低60%）
- **v5配置**: beta = 5.0e-5（降低80%）

**原因**: 
- beta控制机制z的正则化强度，beta越小，机制越灵活
- Inductive数据集需要更强的泛化能力，降低beta可以提升模型对未见数据的适应性
- 从对比结果看，Inductive数据集表现最差，需要更灵活的机制

### 2. 调整学习率
- **当前配置**: lr = 4.0e-4
- **v4配置**: lr = 3.5e-4（降低12.5%）
- **v5配置**: lr = 3.0e-4（降低25%）

**原因**:
- 较低的学习率可以带来更稳定的训练
- 配合更长的训练轮次，可以更好地收敛
- 避免训练后期震荡，提升最终性能

### 3. 增强权重衰减
- **当前配置**: weight_decay = 1.0e-5
- **v4配置**: weight_decay = 2.0e-5（增加100%）
- **v5配置**: weight_decay = 2.5e-5（增加150%）

**原因**:
- 更强的正则化可以防止过拟合
- 特别对Inductive数据集，需要更好的泛化能力
- 权重衰减可以帮助模型在多个图之间找到更好的平衡

### 4. 增加训练轮次
- **当前配置**: num_epoch = 25
- **v4配置**: num_epoch = 30（增加20%）
- **v5配置**: num_epoch = 35（增加40%）

**原因**:
- 更长的训练可以让模型充分收敛
- 特别是对Inductive数据集，需要更多轮次来学习泛化特征
- 配合较低的学习率，可以逐步优化到更好的局部最优

### 5. 增加batch_per_epoch
- **当前配置**: batch_per_epoch = 1000
- **v4配置**: batch_per_epoch = 2000（增加100%）
- **v5配置**: batch_per_epoch = 2500（增加150%）

**原因**:
- 更多的batch可以提升训练稳定性
- 每个epoch看到更多数据，有助于多图预训练
- 减少训练过程中的方差，提升最终性能

### 6. 增加对抗温度（仅v5）
- **当前配置**: adversarial_temperature = 1
- **v5配置**: adversarial_temperature = 1.2（增加20%）

**原因**:
- 更高的对抗温度可以让模型更关注困难样本
- 有助于提升Hits@3等指标
- 特别对Inductive数据集，困难样本更多，需要更强的学习

## 配置对比

| 参数 | 当前(v1) | v4 | v5 |
|------|-----------|----|----|
| beta | 2.5e-4 | 1.0e-4 | 5.0e-5 |
| lr | 4.0e-4 | 3.5e-4 | 3.0e-4 |
| weight_decay | 1.0e-5 | 2.0e-5 | 2.5e-5 |
| num_epoch | 25 | 30 | 35 |
| batch_per_epoch | 1000 | 2000 | 2500 |
| adversarial_temperature | 1.0 | 1.0 | 1.2 |

## 预期效果

### v4配置（保守优化）
- 预期MR下降3-5%
- 预期MRR提升0.5-1%
- 预期Hits@3提升1-2%
- 重点改善Inductive数据集表现

### v5配置（激进优化）
- 预期MR下降5-8%
- 预期MRR提升1-2%
- 预期Hits@3提升2-3%
- 大幅改善Inductive和Ingram数据集表现
- 可能对Transductive数据集略有影响（但整体提升）

## 使用建议

1. **先尝试v4配置**: 如果训练稳定，性能提升明显，则使用v4
2. **如果v4效果不够**: 再尝试v5配置，但需要更长的训练时间
3. **监控指标**: 重点关注Inductive数据集的MR和Hits@3指标
4. **早停策略**: 如果验证集MRR连续5个epoch不提升，可以考虑早停

## 训练命令

```bash
# v4配置
python src/pretrain_relation.py -c ./config/pretrain_optimized_v4.yaml --gpus [0]

# v5配置
python src/pretrain_relation.py -c ./config/pretrain_optimized_v5.yaml --gpus [0]
```

## 注意事项

1. v5配置训练时间会更长（35轮 vs 25轮）
2. 更低的beta可能导致训练初期不稳定，需要监控loss曲线
3. 如果出现NaN或loss爆炸，可以适当提高beta值
4. 建议使用wandb监控训练过程



